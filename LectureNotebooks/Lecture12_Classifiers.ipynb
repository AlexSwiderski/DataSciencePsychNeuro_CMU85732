{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to basic classification\n",
    "\n",
    "Acts:\n",
    "- Logistic regression\n",
    "- LDA\n",
    "- QDA\n",
    "\n",
    "The lecture draws from Chapter 4 of James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). \"An introduction to statistical learning: with applications in r.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**: a categorical response variable - recall accuracy, disease diagnosis, etc\n",
    "\n",
    "**Question**: Why not linear regression?\n",
    "- if you code categories with numbers, e.g. 1, 2, 3, you *could* run a linear regression, but the results will be uninterpretable - there is no particular reason why one category is assigned the number 3, rather than 2, but linear regression assumes that 3 is not only more than 2, but that the difference between 2 and 3 is the same as that between 1 and 2. For example, if you are trying to determine which disease is consistent with a set of symptoms it does not make sense to say that disease A is more than disease B\n",
    "- for a binary case, for example, correct vs incorrect recall, you could code corrects as 1 and incorrect as 0 and run a linear regression to estimate the probability of recall. However, your linear model, as in the figure below might predict probability less than 0 or more than 1, which is impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear regression on binary data](imgs/L12Fig1a.png)\n",
    "\n",
    "**[*Note*:** since ANOVA is just a special case of linear regression, it is also inappropriate for analyzing proportion data, although this is a very common practice]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Logistic Regression model \n",
    "*(supervised method, parametric, classification)*\n",
    "\n",
    "Let's focus on the binary case, because it is the most common one. Consider the stop-signal task paradigm, in which participants have to identify a stimulus with a button press, but some of the stimuli are followed by a \"stop signal\" which instructs participants to withold response. This stop signal occurs after variable intervals, and we would like to figure out the probability of succesfull stopping for a given interval. \n",
    "\n",
    "If we apply a linear regression model, as in the figure above, we do see that it generally captures the fact that stopping is more likely as the interval increases. However, it predicts negative probability for fast intervals and at maximum it predicts ~40% probability even for the slowest speeds, for which we can see that the data shows alsmost exclusively succesfull stopping. \n",
    "\n",
    "What we need in this case is a model function that would make predictions bound between 0 and 1. The most common one is the logistic or binomial function shown on the figure below:\n",
    "\n",
    "![linear regression on binary data](imgs/L12Fig1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are not modelling Y ~ f(X), as we are doing with linear regression. Rather, we are estimating the probability of Y being a specific category, depending on the value of x or:\n",
    "\n",
    "$$P(Y = 1  |  X)$$ or $$P(Y = 1) \\sim f(X)$$ or simply $$p(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function is specifically:\n",
    "$$ p(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than estimating p(X), however, we could rewrite this function it terms of the odds of Y being a specific category:\n",
    "\n",
    "$$odds(Y=1) = \\frac{p(X)}{1-p(X)} = e^{\\beta_0+\\beta_1X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could take the logarithm of the odds, known as the log oddds, and we get a function similar to that of linear regression.\n",
    "\n",
    "$$\\log\\bigg(\\frac{p(X)}{1-p(X)}\\bigg) = \\beta_0+\\beta_1X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio ranges from 0 to Inf, and the log odds range between -Inf and +Inf, so in this case they could be a linear function of X without the issues outlined above. \n",
    "\n",
    "---\n",
    "### Interpretation of logistic regression coefficients\n",
    "Remember, for linear regression, a beta value of 0.5 means that one unit change in X is associated with 0.5 units change in Y. For logistic regression the interpretation is not as direct - one unit change in X is a 0.5 unit change in the log odds of Y being equal to 1.\n",
    "\n",
    "There is no straightforward way to transform this into unit changes in p(X) directly. Why, after all we have a formula, right? The problem is that addition turns into multiplication in exponential space. E.g.:\n",
    "\n",
    "$$e^{\\beta_0+\\beta_1X} = e^{\\beta_0}e^{\\beta_1X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, how much p(X) will change as one predictor in X increases by one unit will depend on the values of the other predictors. You can also see that in the S-shape form of the function above. \n",
    "\n",
    "*In general, however, you can interpret the direction and relative size of coefficients the same way you do in linear regression* - positive coefficients means that p(X) increases as X increases, and bigger beta values imply a bigger increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Classification threshold\n",
    "\n",
    "So far, aside from the transformation on the left side of the function, the logistic model is very similar to the linear regression model. Where does classification come in? \n",
    "\n",
    "The logistic regression model makes predictions about the probability of Y being one of two categorical values. We can classify Y based on:\n",
    "\n",
    "1. Prediction of the model\n",
    "2. A threshold\n",
    "\n",
    "For example, if we chose a threshold of 0.5, then we would classify Y = 1 for any p(X) >= 0.5, and as Y = 0 for any p(X) < 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Estimating parameters\n",
    "\n",
    "In contrast to linear regression, logistic regression does not have a closed form solution like least squares. Logistic regression paramters have to be estimated with maximum likelihood techniques. Conceptually, the goal is to find $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ such that plotting $\\hat{y}$ is close to 1 for all values where $y=1$ and close to zero for all values where $y = 0$. \n",
    "\n",
    "The **likelihood function** that has to be optimized is:\n",
    "\n",
    "![Likelihood function](imgs/L12Fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood will be highest when the probabilities of the corresponding values of X for y=1 and of X for y=0 are highest under the corresponding beta values.Thus, this respresents the likelihood of these coefficients given the model and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiple predictors\n",
    "\n",
    "The logistic regression model is nice as it expands out to multiple predictor variables quite easily, in the same way as linear regression:\n",
    "\n",
    "$$ \\log\\bigg(\\frac{p(X)}{1-p(X)}\\bigg) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$$\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1+e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "- y = probability of getting the flu\n",
    "- x1 = age\n",
    "- x2 = occupation (1 if teacher, 0 otherwise)\n",
    "- x3 = social network size\n",
    "\n",
    "Inferring the significance of each regression coefficient works exactly the same now as linear regression:\n",
    "$$t=\\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When is logistic regression not the most suitable method?\n",
    "\n",
    "- More than 2 classes\n",
    "- Parameter estimates from logistic regression become unstable when the classes are well separated.\n",
    "- If n is small and the predictors are normally distributed, parameter estimates are less stable than other methods\n",
    "\n",
    "When logistic regression can't be used due to these reasons, we can use several other methods. In the next section we'll focus on Linear discriminant analysis (LDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Linear discriminant analysis (LDA) \n",
    "*(unsupervised method, parametric, classification)*\n",
    "\n",
    "In contrast to logistic regression, which models p(X) as a function f(X), LDA models the distribution of X scores separately for each category of Y. Thus, it obtains the probability distribution of X given a specific value of Y, $p(X | Y =k)$. It then uses Bayes theorem to arrive at the desired probability, namely the probability of classifying Y to a category k given a specific value of the predictor X, $P(Y=k|X)$.\n",
    "\n",
    "- *Note:* When distributions are normal, this becomes a special case of logistic regression.\n",
    "\n",
    "Thus, LDA find the best way to carve up your data into meaningful groups. One the left figure below, we see the distribution of some predictor X, conditional on the category of Y. On the right, we see the same distribution for random samples drawn from X.\n",
    "\n",
    "![LDA](imgs/L12Fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bayes theorem for classification:\n",
    "- Bayes theorem:\n",
    "$$ P(A|B) = \\frac{p(B|A)p(A)}{p(B)}$$\n",
    "\n",
    "    - likelihood: $p(B|A)$\n",
    "    - prior: $p(A)$\n",
    "    \n",
    "- Example: What's the probability of it having rained if the sidewalk is wet? Well, it's a function of how likely it is to rain in the first place, how likely it is to be wet if it had in fact rained, and how likely it is to be wet due to other reasons. Specifically:\n",
    "\n",
    "$$ P(rain|wet) = \\frac{p(wet|rain)p(rain)}{p(wet)}$$\n",
    "\n",
    "- As you can see, the probability of having rained given that it's wet outside:\n",
    "    - increases if you live in a rainy country (UK) and decreases if you live in Israel where it rarely rains and wetness is more likely due to other reasons\n",
    "    - decreases if there are many likely reasons of it being wet (p(wet)) - if you live in 12th century, people used to empty latrines through the window on the street, which we don't do any more\n",
    "    - increases if the rain was recent (p(wet|rain))\n",
    "\n",
    "\n",
    "- Goal: Classify any observation yiinto one of any K classes.\n",
    "- $\\pi_k$ = prior probability that a given observation is associated with the kth category\n",
    "- Frequentist vs. Bayes (point number versus probability distribution)\n",
    "- **Density function:** \n",
    "$$f_k(X) \\equiv P(X = x | Y = k)$$\n",
    "    - $f_k(X)$ is relatively large if there is a high probability that an observation in the kth class has X~= x\n",
    "- Approximation of Bayes Theorem:\n",
    "$$P(Y = K|X=X) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)}$$\n",
    "- Posterior probability:\n",
    "$$ p_k(X) = P(Y = k|X) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The case with one predictor (p = 1):\n",
    "- Have to assume shape of $f_k(X)$\n",
    "- For example, Gaussian distribution:\n",
    "$$f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\bigg(-\\frac{1}{2\\sigma^2_k}(x-\\mu_k)^2\\bigg)$$\n",
    "- which gives an p_k as:\n",
    "$$p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2\\bigg)}{\\sum_{l=1}^{K}\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigg)}$$\n",
    "- log(pk(x)) gives us the discriminant function:\n",
    "$$\\delta_k(x) = x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k)$$\n",
    "    - where $\\pi_k = \\frac{n_k}{n}$\n",
    "- **Bayes decision boundary: The point that separates the two distributions    **\n",
    "$$x = \\frac{\\mu_1^2-\\mu_2^2}{2(\\mu_1-\\mu_2)} = \\frac{\\mu_1+\\mu_2}{2}$$\n",
    "- Discriminant function: the posterior probability estimate that x is in class K.\n",
    "- LDA tries to find best parameters of k,k, and k as model parameters\n",
    "![LDA parameters](imgs/L12Fig4.png)\n",
    "    - By plugging in the parameter estimates you get the discriminant function for each k category:\n",
    "$$\\hat{\\delta}_k(x) = x\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k)$$\n",
    "    - Higher discriminant values indicate more likely x is a member of the kth group.\n",
    "- Algorithm:\n",
    "    - $Z = w_1x_1 + ... + wx_p$\n",
    "    - $S(\\beta) = (w^T\\mu_1-w^T\\mu_2)/(w^T\\sum\\beta)$\n",
    "    - Model coefficients: $w = \\sum^{-1}(\\hat{\\mu}_1-\\hat{\\mu}_2)$\n",
    "    - Pooled covariance: $\\sum = (n_1+n_2)^{-1}(n_1\\sum_1+n_2\\sum_2)$\n",
    "    - Objective function: $w^T(X-((\\hat{\\mu}_1+\\hat{\\mu}_2)/2)) > \\log(p(k = 1)/p(k = 0))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The case with multiple predictors (p > 1)\n",
    "\n",
    "It's easy to extend the LDA approach to multiple predictors. In this case, we are using the multivariate GAussian density function:\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{(2\\pi)^p|\\Sigma|}}\\exp\\bigg(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\bigg)$$\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix for all K classes.\n",
    "\n",
    "The discriminat function becomes:\n",
    "$$\\delta_k(x) = x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k+log\\pi_k$$\n",
    "\n",
    "which is just a vectorized version of the discriminant function for p=1\n",
    "\n",
    "This results in the following decision boundaries in the example figure below:\n",
    "\n",
    "![LDA decision boundaries for p=2](imgs/L12fig5.png)\n",
    "\n",
    "- Note that there are multiple decision boundaries (one for every pair of k categories)\n",
    "- Dashed lines show real bayes decision boundaries\n",
    "- Solid lines show the LDA decision boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LDA as a multi-purpose classifier\n",
    "\n",
    "- Prediction problem: Build a model (Train), then test that model on new data (Test)\n",
    "- Beware biases:\n",
    "\n",
    "    - Overfitting: When p > n, then you will do well because you fit on noise.\n",
    "    - Proper knull: E.g., in the default example, the default rate is low, so just guessing that no one will default gives you high accuracy.\n",
    "        - Null is not 50/50\n",
    "    - Consider the example in the book\n",
    "\n",
    "![Classifier table](imgs/L12fig6.png)\n",
    "\n",
    "Classifier performance evaluated against two properties:\n",
    "\n",
    "- Sensitivity: percentage of true positives identified\n",
    "- Specificity: percentage of false negatives avoided\n",
    "- Playing with decision thresholds can impact sensitivity & specificity of classifier\n",
    "\n",
    "**The ROC curve:**\n",
    "- Receiver operating characteristic. Started with radio signals\n",
    "- Plot the False alarms (x axis) against the hits (y axis)\n",
    "- Take into account all possible thresholds\n",
    "\n",
    "![ROC fig](imgs/L12ROCfig.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
